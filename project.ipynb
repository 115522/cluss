{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">  <B> Tweets Clustering</B>  $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$<B> BY INES REBHI</B>\n",
    " </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "html1 = '<img src=\"images/giphy.gif\" width=\"400\" height=\"400\" align=\"center\"/>'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\"> <B>Introduction</B>\n",
    " </B> </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Data redundancy is an important problem of Twitter. Twitter users are likely to generate similar tweets (e.g., using the Retweet function) about some popular topics/events.\n",
    "a result of a huge number of tweets which let  tweetos not interested to loss time about reading for the same topic many tweets     \n",
    "  So by clustering similar tweets together, we can generate a more concise and organized representation of the raw tweets, which will be very useful for busy Tweetos to read only one tweet per class  </B> </br>\n",
    "\n",
    "<B> Aim of our Project is to  </B>\n",
    "<li> <B>1/ gathering real time tweets  using a twitter API </B></li>\n",
    "<li><B> 2/ Preprocessing tweets </B> </li>\n",
    "<li><B> 3/Apply a NLP to text tweets</B> </li>\n",
    "<li><B> 4/ Modeling By using K-means as a ML algorithm of clustering </B></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\"> <B>Keywords </B></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Text mining  $~~~/~~~~$clustering$~~~/~~~~$NLP $~~~/~~~~$tweepy$~~~/~~~~$ NLTK$~~~/~~~~$ twitter API </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\"> <B>STEP1: Getting API keys from Twitter</B>\n",
    " </B> </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>An API is standing for Application Programming Interfaces (APIs) and they allow you to access resources only available on the server</B></br>\n",
    "<B>Now The Twitter API lets you read and write Twitter data. Thus, you can use it to compose tweets, read profiles, and access your followers’ data and a high volume of tweets on particular subjects in specific locations.</B></br>\n",
    "<B>So how can I get an API from twitter developer ??? </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<B style=\"color:orangered;\">1/ Create  twitter account </B>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/accounttwitter.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\">2/Try this link to see how apply for a twitter api </B> </br></br>\n",
    "\n",
    "<B>https://www.youtube.com/watch?v=vlvtqp44xoQ </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\">3/ Create a project in twitter developer</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Capture.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\">3/Get API  Keys </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/api.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\"> 3/ Install Tweepy </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Tweepy is an open source Python package that gives you a very convenient way to access the Twitter API with Python. Tweepy includes a set of classes and methods that represent Twitter’s models and API endpoints, and it transparently handles various implementation details, such as:</B>\n",
    "\n",
    "<B>* Rate limits</B>\n",
    "    \n",
    "<B>* Streams</B>\n",
    "\n",
    "<B> * Data encoding and decoding</B>\n",
    "    \n",
    "<B> * HTTP requests</B>\n",
    "    \n",
    "<B>* Results pagination</B>\n",
    "    \n",
    "<B>* OAuth authentication</B>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\">4/ import Libraries  and credentials in our project </B> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy           # To consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For number computing\n",
    "\n",
    "# For plotting and visualization:\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Twitter App access keys for @user \n",
    "#credential part\n",
    "\n",
    "# Consume:\n",
    "CONSUMER_KEY    = 'pUruES5UQBACkbkcOolgfmjop'\n",
    "CONSUMER_SECRET = 'H0zY8T19bQIAMGKxTNEVZV4T52QNMfruGu8FLbECmizbw7qr7l'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = '1324988873538228224-Sbn1EJXcxKeItAV33UzO7wX7TsFYjA'\n",
    "ACCESS_SECRET = 'MMyMliA8oNgEYspY3mKQ4Wi73F4YsD9CiELkyChQpxebo'\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN,ACCESS_SECRET)\n",
    "api=  tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><B style=\"color:blue;\">STEP2: Gathring DATA  </B> </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\">1/ Live streaming tweets\n",
    "  </B> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>In this Part we need to stream 10 000 tweets in real-time  from\n",
    "Twitter to solve the task of tweets clustering </B> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Danger:</b> <B>Please don't run this cell because it can make you a lot of time to do the streaming ! </B> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    " \n",
    "class listener(StreamListener):\n",
    "    def on_data(self,data):\n",
    "       \n",
    "            saveFile = open('live_tweets1.json','a')\n",
    "            saveFile.write(data)\n",
    "            saveFile.write('\\n')\n",
    "            saveFile.close()\n",
    "            return True\n",
    "       \n",
    "            time.sleep(5)\n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            if hasattr(status, 'retweeted_status') and hasattr(status.retweeted_status, 'extended_tweet'):\n",
    "                print('retweeted: ' + status.retweeted_status.extended_tweet['full_text'])\n",
    "            if hasattr(status, 'extended_tweet'):\n",
    "                print('extended_tweet: ' + status.extended_tweet['full_text'])\n",
    "            else:\n",
    "                print('text: ' + status.text)\n",
    "        except AttributeError:\n",
    "            print('attribute error: ' + status.text)\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN,ACCESS_SECRET)\n",
    "twitterStream = Stream(auth,listener(),tweet_mode='extended')\n",
    "\n",
    "twitterStream.filter(languages=[\"en\"],track=[\"politic\" , \"health\" , \"sport\",\"business\",\"entertaiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><B style=\"color:blue;\">STEP3:DATA PREPARATION </B> </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h3><B style=\"color:orangered;\">Data Processing and Wrangling  </B> </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">><B style=\"color:green;\"> 1/ From json to csv </B> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>the csv form  serve to make our dataset(tweets) more visualised than json form. So we need to change our dataset from json to csv </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"info info-block alert-info\">\n",
    "<b>INFO:</b> <B>It's unused to run these two cells </B> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json ('tweetsstreamed.json',lines = True)\n",
    "export_csv = df.to_csv ('bdtweets.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json ('live_tweets.json',lines = True)\n",
    "export_csv = df.to_csv ('bdtweets2.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">><B style=\"color:green;\"> 2/Cleaning text </B> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">><B>In this step we will \n",
    "<B><li>transform tweet text into lowercase</li></B>\n",
    "<B><li>remove twitter handles</li></B>\n",
    "<B><li>remove hyperlinks</li></B>\n",
    "    <B><li>remove non-alphanumeric characters such as punctuation marks </li></B>\n",
    "<B><li>remove whitespace</li></B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>a/keep only the text column and ignore the rest of features</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tweets dataframe with duplicated tweets (20422, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@WTAJnews Great!  Another going out of busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DailyCaller @LisaMarieBoothe Health and educa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @MatthewJshow: @realDonaldTrump China is we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video: Demba Ba sees red as he stands up for h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @texman71: Fuck your restrictions!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @WTAJnews Great!  Another going out of busines...\n",
       "1  @DailyCaller @LisaMarieBoothe Health and educa...\n",
       "2  RT @MatthewJshow: @realDonaldTrump China is we...\n",
       "3  Video: Demba Ba sees red as he stands up for h...\n",
       "4             RT @texman71: Fuck your restrictions!!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "tweets = pd.read_csv(\"bdtweets2.csv\", usecols = ['text'])\n",
    "print(\"shape of tweets dataframe with duplicated tweets\",tweets.shape)\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20417</th>\n",
       "      <td>TAKE ACTION AND OPPOSE RUSHED APPROVALS WITHOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20418</th>\n",
       "      <td>Revive your aging skin and reduce the appearan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20419</th>\n",
       "      <td>Busy 🐝!!!! \\n\\nWhen you own a business this is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20420</th>\n",
       "      <td>RT @JeaneF1MSP: Dear @BBCScotNine, unpaid care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20421</th>\n",
       "      <td>RT @hycfreedom: @HBO @HackedOffHugh HUNTER BID...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "20417  TAKE ACTION AND OPPOSE RUSHED APPROVALS WITHOU...\n",
       "20418  Revive your aging skin and reduce the appearan...\n",
       "20419  Busy 🐝!!!! \\n\\nWhen you own a business this is...\n",
       "20420  RT @JeaneF1MSP: Dear @BBCScotNine, unpaid care...\n",
       "20421  RT @hycfreedom: @HBO @HackedOffHugh HUNTER BID..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RT @SenTomCotton: This opinion reads like the delusional ramblings of a Resistance lawyer afflicted with Trump Derangement Syndrome. Judge…         186\n",
       "RT @btsportfootball: Nothing but respect ❤️\\n\\nMessi 🤝 Ronaldo\\n\\n📺 BT Sport ESPN HD https://t.co/Qo1KUQKZ7Q                                        171\n",
       "RT @SteveGuest: Joe Biden defeated AGAIN by the Teleprompter.\\n\\n\"For Secretary of Health and Education Services, I nominate Xavier Bacheria.…      153\n",
       "RT @MelissaReddy_: Demba Ba 👏🏽\\nIstanbul Basaksehir 👏🏽\\nPSG 👏🏽\\n\\nShowing us 'zero tolerance' is more than just empty words, can be easily acti…    137\n",
       "RT @SchmittNYC: Biden didn’t nominate Becerra - his handlers did                                                                                    120\n",
       "                                                                                                                                                   ... \n",
       "RT @nickchodgson: Another great interview by @HelenDon_RCN on nursing staff’s leading role in the #COVID19Vaccine rollout and astute questi…          1\n",
       "Mental health is a topic many people try to dance around, however we really need to focus more on changing how we v… https://t.co/XBh1lfsesu          1\n",
       "@realDonaldTrump You’re an embarrassment. And should be seeking mental health help.                                                                   1\n",
       "All my patriot friends I need help. My mother inlaw's family's business burned down and they desperately need help.… https://t.co/SYakxABtIJ          1\n",
       "@NaijaPledgeEnt @Irunnia_ Can you mind your business and stop watching the game from behind the tv?                                                   1\n",
       "Name: text, Length: 13937, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><B>we are looking in our dataset and  note that we have many duplications that we must removing</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>b/Remove duplicated tweets </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tweets dataframe After removing duplicated tweets (13937, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets.drop_duplicates(subset='text',inplace=True)\n",
    "# tweets.drop_duplicates(subset=['text'], keep='first', inplace=True)  #tou can also run this line of code to drop duplicate tweets\n",
    "\n",
    "print(\"shape of tweets dataframe After removing duplicated tweets\",tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><B> we can showing top of hashtag in our dataset </B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tweets['hashtags'] = tweets['text'].apply(lambda twt : re.findall(r\"#(\\w+)\", twt))\n",
    "d = Counter(tweets.hashtags.sum())\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3602"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_hashtags = pd.DataFrame([d]).T\n",
    "tweets_hashtags.columns = ['freq']\n",
    "tweets_hashtags.freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top hashtag in our dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COVID19</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignment</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homework</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nursing</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Literature</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Essay</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chemistry</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            freq\n",
       "COVID19      103\n",
       "business      81\n",
       "health        42\n",
       "assignment    38\n",
       "homework      33\n",
       "Nursing       31\n",
       "research      31\n",
       "Literature    29\n",
       "Essay         28\n",
       "chemistry     26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_hashtags.sort_values(by=['freq'], ascending=False, inplace=True)\n",
    "print(\"top hashtag in our dataset\")\n",
    "tweets_hashtags.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 20000 Tweets, 3602 Hashtags were used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Top 10 Hashtags of dataset')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualization of Top 10 Hashtags\n",
    "labels = tweets_hashtags.head(10).index.values.tolist()\n",
    "freq = tweets_hashtags['freq'].head(10).values.tolist()\n",
    "index = np.arange(len(freq))\n",
    "\n",
    "print(\"Among 20000 Tweets, 3602 Hashtags were used.\")\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.bar(index, freq, alpha=0.8, color= 'orangered')\n",
    "plt.xlabel('Hashtags', fontsize=13)\n",
    "plt.ylabel('Frequency', fontsize=13)\n",
    "plt.xticks(index, labels, fontsize=11, rotation=90, fontweight=\"bold\") \n",
    "plt.title('Top 10 Hashtags of dataset', fontsize=12, fontweight=\"bold\",color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['hashtags'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <B> c/extract the username through the tweets (preceded by @ or by RT @), emojis,punctuation</B></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extended_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great  another going out of business sale</td>\n",
       "      <td>great  another going out of business sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health and education services is what he wan...</td>\n",
       "      <td>health and education services is what he wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>china is well entrenched throughout business...</td>\n",
       "      <td>china is well entrenched throughout business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video demba ba sees red as he stands up for hi...</td>\n",
       "      <td>video demba ba sees red as he stands up for hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck your restrictions</td>\n",
       "      <td>fuck your restrictions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20416</th>\n",
       "      <td>we haveright to education and good health we w...</td>\n",
       "      <td>we haveright to education and good health we w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20417</th>\n",
       "      <td>take action and oppose rushed approvals withou...</td>\n",
       "      <td>take action and oppose rushed approvals withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20418</th>\n",
       "      <td>revive your aging skin and reduce the appearan...</td>\n",
       "      <td>revive your aging skin and reduce the appearan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20419</th>\n",
       "      <td>busy  when you ownbusiness this is really the ...</td>\n",
       "      <td>busy  when you ownbusiness this is really the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20421</th>\n",
       "      <td>hunter biden lied tony bobulinski  the form...</td>\n",
       "      <td>hunter biden lied tony bobulinski  the form...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13937 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0              great  another going out of business sale   \n",
       "1        health and education services is what he wan...   \n",
       "2        china is well entrenched throughout business...   \n",
       "3      video demba ba sees red as he stands up for hi...   \n",
       "4                                 fuck your restrictions   \n",
       "...                                                  ...   \n",
       "20416  we haveright to education and good health we w...   \n",
       "20417  take action and oppose rushed approvals withou...   \n",
       "20418  revive your aging skin and reduce the appearan...   \n",
       "20419  busy  when you ownbusiness this is really the ...   \n",
       "20421     hunter biden lied tony bobulinski  the form...   \n",
       "\n",
       "                                          extended_tweet  \n",
       "0              great  another going out of business sale  \n",
       "1        health and education services is what he wan...  \n",
       "2        china is well entrenched throughout business...  \n",
       "3      video demba ba sees red as he stands up for hi...  \n",
       "4                                 fuck your restrictions  \n",
       "...                                                  ...  \n",
       "20416  we haveright to education and good health we w...  \n",
       "20417  take action and oppose rushed approvals withou...  \n",
       "20418  revive your aging skin and reduce the appearan...  \n",
       "20419  busy  when you ownbusiness this is really the ...  \n",
       "20421     hunter biden lied tony bobulinski  the form...  \n",
       "\n",
       "[13937 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "#clean text\n",
    "\n",
    "def CleanText(TextToClean):\n",
    "    TextToClean.lower()  #Normalization\n",
    "    TextToClean = re.sub(r'@[A-Za-z0-9]+', '',str(TextToClean)) #to extract @\n",
    "    TextToClean= re.sub(r':','',str(TextToClean)) #to remove :\n",
    "    TextToClean= re.sub(r'\\n','',str(TextToClean)) #to remove  \\n\n",
    "    TextToClean= re.sub(r'#','',str(TextToClean)) #to remove #\n",
    "    TextToClean= re.sub(r'!','',str(TextToClean)) #to remove #\n",
    "    TextToClean = re.sub(r'RT[\\s]','',str(TextToClean)) #to remove #\n",
    "    TextToClean = re.sub(r'https','',str(TextToClean)) #to remove hyper links\n",
    "    TextToClean = re.sub(r'_','',str(TextToClean)) #to remove hyper links\n",
    "    TextToClean = re.sub(r'~~~@…','',str(TextToClean)) #to remove blanc space\n",
    "    TextToClean =re.sub(r\"\\&\\S*\\s\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\&\", \"\", str(TextToClean))\n",
    "    TextToClean= re.sub(r\"\\+\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\#\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\$\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\£\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\%\", \"\", str(TextToClean))\n",
    "    TextToClean= re.sub(r\"\\:\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\@\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\-\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\",\", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\" \\' \", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\"\\n \", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r'\\s+[a-zA-Z]\\s+', \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r\" \\? \", \"\", str(TextToClean))\n",
    "    TextToClean = re.sub(r'[^\\w\\s]','',str(TextToClean))\n",
    "    \n",
    "#     TextToClean = re.sub(r'\\s+', \"\", str(TextToClean))#Removing Extra Whitespaces\n",
    "#     TextToClean = re.sub(r'\\s+[a-zA-Z]\\s+', \"\", str(TextToClean))#Removing single carecter\n",
    "\n",
    "\n",
    "    return TextToClean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_emoji(Text_with_emoji):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (appel phone)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', Text_with_emoji)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(CleanText)\n",
    "tweets['text'] = tweets['text'].apply(remove_emoji)\n",
    "# tweets['extended_tweet'] = tweets['text'].apply(CleanText)\n",
    "# tweets['extended_tweet'] = tweets['text'].apply(remove_emoji)\n",
    "#lower text\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "# tweets[\"extended_tweet\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' new scathing analysis of s shady business by the  thereslot of questions around this guy the'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b></b> <B> Now ! we have only cleaned and lower text </B>  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\"> <B>  Natural Language Processing </B>  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Natural language processing includes many different techniques for interpreting human language, ranging from statistical and machine learning methods to rules-based and algorithmic approaches.</B>\n",
    "</br>\n",
    "<B>Basic NLP tasks include tokenization and parsing, lemmatization/stemming, part-of-speech tagging, language detection and identification of semantic relationships.</B></br>\n",
    "<B>In general terms, NLP tasks break down language into shorter, elemental pieces, try to understand relationships between the pieces and explore how the pieces work together to create meaning.</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\"color:orangered;\"> <B> Natural Language Toolkit (NLTK) for  analyzing text </B>  </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>NLTK is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Install NLTK </B></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434679 sha256=3bc961ddc00caade782f8044f76ddea6f35277e2fc0e79d914c76b880c08c97e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\ff\\d5\\7b\\f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Install Conda package for NLTK </B></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"images/anacondaNLTK.PNG\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B>Install punkt</B></h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/punketinstalled.PNG\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "><h4 style=\"color:orangered;\"><B> remove stopwords </B></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<B>In every language, some words are particularly common. While their use in the language is crucial, they don’t usually convey a particular meaning, especially if taken out of context. This is the case of articles, conjunctions, some adverbs, etc. which are commonly called stop-words.</br>\n",
    "Stop-word removal is one important step that should be considered during the pre-processing stages. </br>\n",
    "NLTK provides a simple list for English stop-words. </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stopwords.PNG\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Tokenization </B></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>In this step , we will split tweets text into token (words) by using NLTK’s TweetTokenizer.It can  “collapse” repeated characters - that is, lolll, lollllll, and lollllllllllll will all collapse to the same representation \"lolll\" (three “l”s). This is helpful because we tend to think that these tokens represent approximately the same thing. This feature helps curb the curse of dimensionality (i.e. too many low-frequency tokens), while maintaining Twitter-specific features.  </B>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'under', 'our', 'for', 'has', 'does', 'that', 'over', 'nor', 'mustn', 'doing', \"wasn't\", 'now', 'until', 've', 'ours', 'shouldn', 'can', 's', 'will', 'having', 'from', 'this', 'what', 'off', 'shan', 'down', 'herself', 'too', 'ourselves', 'few', 'did', \"didn't\", 'ma', 'same', 'who', 'being', 'hadn', 'or', 'weren', 'where', 'through', 'them', \"needn't\", 'his', 'those', \"wouldn't\", 'd', 'aren', 'such', 'into', \"mightn't\", 'a', 'how', 're', 'you', 'at', 'on', 'and', 'of', 'are', 'have', 'between', 'than', 'in', 'ain', 'so', 'we', 'to', 'am', 'won', 'just', 'very', 'yours', 'here', 'while', 'your', \"should've\", \"you're\", 'been', 'doesn', 'up', 'again', 'but', 'hers', 'yourself', 'not', 'couldn', 'all', 'myself', 'about', 'him', 'during', 'her', 'most', 'isn', \"hadn't\", \"it's\", 'my', 't', \"aren't\", 'their', \"shouldn't\", 'is', \"you've\", 'more', 'then', 'didn', 'do', 'it', \"won't\", 'y', 'had', 'which', 'itself', \"weren't\", \"haven't\", 'haven', 'only', 'these', 'there', 'mightn', 'needn', 'theirs', 'when', 'above', 'm', \"isn't\", 'rts', 'any', \"hasn't\", 'should', 'yourselves', 'she', 'below', \"couldn't\", 'further', \"she's\", 'o', 'before', 'against', \"don't\", 'i', \"shan't\", 'me', 'be', 'its', 'both', 'an', 'no', 'own', 'other', \"doesn't\", 'he', 'because', 'after', 'as', 'each', \"mustn't\", 'the', 'hasn', 'll', 'some', 'if', 'by', 'once', \"you'd\", 'were', 'why', 'wasn', 'themselves', 'whom', 'they', \"that'll\", 'retweet', 'don', 'wouldn', 'rt', 'with', 'out', 'was', \"you'll\"}\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************************************\n",
    "                                               #This is the stopwords\n",
    "#*************************************************************************************************\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "additional  = ['rt','rts','retweet']\n",
    "swords = set().union(stopwords.words('english'),additional)\n",
    "print(swords)\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(lambda x: ' '.join([i for i in x.split() if  i not in (swords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new scathing analysis ofshady business thereslot questions around guy'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************************************************\n",
    "                                               #This is the tokenizer\n",
    "#*************************************************************************************************\n",
    "\n",
    "\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "tweets['tokonized_text'] = '' \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tweets['tokonized_text']  =tweets['text'].apply(tokenizer.tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'scathing',\n",
       " 'analysis',\n",
       " 'ofshady',\n",
       " 'business',\n",
       " 'thereslot',\n",
       " 'questions',\n",
       " 'around',\n",
       " 'guy']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokonized_text'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Stemming </B></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "tweets['stemmed'] = tweets['tokonized_text'].apply(lambda x: [ps.stem(i) for i in x if i != ''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'scath',\n",
       " 'analysi',\n",
       " 'ofshadi',\n",
       " 'busi',\n",
       " 'thereslot',\n",
       " 'question',\n",
       " 'around',\n",
       " 'guy']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['stemmed'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Lemmatization  </B></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet') #download wordnet package with code \n",
    "lmtzr = WordNetLemmatizer()\n",
    "tweets['lemma'] = tweets['stemmed'].apply(lambda x: [lmtzr.lemmatize(word,'v') for word in x if word!= ''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'scath',\n",
       " 'analysi',\n",
       " 'ofshadi',\n",
       " 'busi',\n",
       " 'thereslot',\n",
       " 'question',\n",
       " 'around',\n",
       " 'guy']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['lemma'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue;\"> <B> STEP4 : Modeling </B>  </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> K-means </B></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Now as a Ml Algorithm for clustering we will use K-means with cosine as a distance metric </B>\n",
    "\n",
    "<B>By this example we will understand How does K-means work </B>\n",
    "\n",
    "<img src=\"images/k-means.png\">\n",
    "<h5 style=\"color:red;\"><B> Step1</B></h5>\n",
    "Here we are having a few data points, which we want to cluster. So we would start by picking the number of clusters we want to have.</br>\n",
    "\n",
    "<h5 style=\"color:purpel;\"><B> Step2</B></h5>\n",
    "We have successfully marked the centers of these clusters. Now we will be marking all the points with respective colors on the basis of the distance they have from the centroid.</br>\n",
    "\n",
    "<h5 style=\"color:purpel;\"><B> Step3</B></h5>\n",
    "After marking all the data points, we will now be computing the centroid of this cluster again. We are doing it because initially, we had picked the centroid randomly. Then to remove error, if any, we are doing it.\n",
    "\n",
    "The centroid of the cluster is computed by finding a point within the cluster that would be equidistant from all the data points.</br>\n",
    "\n",
    "<h5 style=\"color:purpel;\"><B> Step4</B></h5>\n",
    "\n",
    "Now since we have computed the centroid again and we know it is not the same as it was before so we would iterate the process again and would find the points nearest to this centroid for each cluster.</br>\n",
    "\n",
    "<h5 style=\"color:purpel;\"><B> Step5</B></h5>\n",
    "Now we have got the result again. One may ask when shall we stop the iteration of this finding the centroid and then placing the data points accordingly? Well, you have to do it till the time when the position of the centroids doesn’t change.</br>\n",
    "\n",
    "<h5 style=\"color:purpel;\"><B> Step6</B></h5>\n",
    "We marked the two clusters.\n",
    "\n",
    "In this case, it was easy, so we were able to get the results in 2 iterations only.\n",
    "\n",
    "We had also talked about the random initialization that we are putting ourselves into. With this a problem we have is that it can land us up with some really bad clusters which won’t be of any use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> K-means Mertrics</B></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> K-means have 3 distance metrics </B>\n",
    "    <li><B>Euclidean Distance</B></li>\n",
    "    <li><B>Cosine Distance</B></li>\n",
    "    <li><B>Jaccard Similarity</B></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/metricc.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.decomposition import PCA      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Vectorization</B></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig shape: (13937, 33742)\n",
      "new shape: (13937, 100)\n"
     ]
    }
   ],
   "source": [
    "RUNTIME_MODE = \"Cosine\"\n",
    "SAMPLE_COUNT = 14000\n",
    "MAX_CLUSTER = 25\n",
    "random.seed(1)\n",
    "\n",
    "content = tweets['text'].values # Get the news content to news array\n",
    "\n",
    "stop_words = set(text.ENGLISH_STOP_WORDS)\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\n",
    " \"also\", \"said\", \"mr\", \"mrs\", \"im\", \"would\", \"could\", \"should\", \"first\", \"like\", \"dont\",\n",
    " \"wont\", \"get\", \"going\", \"ms\", \"one\", \"____\", \"_____\", \"new\", \"news\", \"told\", \"way\",\n",
    " \"year\", \"years\", \"don\", \"day\", \"man\", \"did\", \"just\", \"time\", \"times\", \"make\", \"000\",\n",
    " \"united\", \"state\", \"states\", \"people\", \"ve\", \"white\", \"house\", \"president\",\n",
    " \"government\", \"york\", \"want\", \"know\", \"think\", \"officials\", \"say\", \"breitbart\",\n",
    " \"percent\", \"home\", \"city\", \"case\", \"really\", \"work\", \"according\", \"including\",\n",
    " \"good\", \"campaign\", \"country\", \"long\", \"world\", \"donald\", \"trump\", \"didn\", \"women\",\n",
    " \"called\", \"american\", \"men\", \"later\", \"follow\", \"week\", \"black\", \"little\", \"company\",\n",
    " \"companies\", \"posted\", \"morning\", \"today\", \"evening\", \"com\", \"nytimes\", \"weekend\", \"10\"\n",
    "]\n",
    ")\n",
    "vectorizer = CountVectorizer(stop_words=my_stop_words, binary=False)\n",
    "X_orig = vectorizer.fit_transform(content[0:SAMPLE_COUNT]).toarray()\n",
    "svd = TruncatedSVD(n_components=100, random_state = 0)\n",
    "X = svd.fit_transform(X_orig)\n",
    "print(\"orig shape:\", X_orig.shape)\n",
    "print(\"new shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> K-means Clustering</B></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\nltk\\cluster\\util.py:131: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item count in clusters:\n",
      "Counter({1: 7707, 0: 6230})\n",
      "Item count in clusters:\n",
      "Counter({1: 7473, 0: 3365, 2: 3099})\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # Get the array of terms that exists in documents\n",
    "inner_dist = np.zeros(shape=[MAX_CLUSTER - 1])\n",
    "for NUM_CLUSTERS in range(2, MAX_CLUSTER + 1):\n",
    "    kmeans = KMeansClusterer(num_means=NUM_CLUSTERS,\n",
    "    distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "#initial_means=means)\n",
    "    labels = kmeans.cluster(X, assign_clusters=True)\n",
    "    cluster_centers = np.asarray(kmeans.means())\n",
    "    print(\"Item count in clusters:\")\n",
    "    print(Counter(labels))\n",
    "    cluster_means = np.zeros(shape=[NUM_CLUSTERS, len(terms)])\n",
    "    item_counts = np.zeros(shape=[NUM_CLUSTERS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names() # Get the array of terms that exists in documents\n",
    "NUM_CLUSTERS = 6\n",
    "inner_dist = np.zeros(shape=[MAX_CLUSTER - 1])\n",
    "kmeans = KMeansClusterer(num_means=NUM_CLUSTERS,distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "#initial_means=means)\n",
    "labels = kmeans.cluster(X, assign_clusters=True)\n",
    "cluster_centers = np.asarray(kmeans.means())\n",
    "print(\"Item count in clusters:\")\n",
    "print(Counter(labels))\n",
    "cluster_means = np.zeros(shape=[NUM_CLUSTERS, len(terms)])\n",
    "item_counts = np.zeros(shape=[NUM_CLUSTERS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Top words per Cluster</B></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(SAMPLE_COUNT):\n",
    "    cluster_means[labels[ind]] = cluster_means[labels[ind]] + X_orig[ind]\n",
    "    item_counts[labels[ind]] = item_counts[labels[ind]] + 1\n",
    "    for ind in range(NUM_CLUSTERS):\n",
    "        cluster_means[ind] = cluster_means[ind] / item_counts[ind]\n",
    "        print(\"Top terms per cluster:\")\n",
    "        order_centroids = cluster_means.argsort()[:, ::-1]\n",
    "\n",
    "    for cls in range(NUM_CLUSTERS):\n",
    "        print(\"Cluster\", cls, ': ', end=''),\n",
    "        for ind in order_centroids[cls, :10]:\n",
    "            print(terms[ind], ' , ', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "vec = CountVectorizer()\n",
    "matrix = vec.fit_transform(tweets['text'])\n",
    "matrix.toarray()\n",
    "\n",
    "# vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h4 style=\"color:green;\"><B> Cosine Metric</B></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.11022302e-16 1.00000000e+00 8.58578644e-01 ... 1.00000000e+00\n",
      "  1.00000000e+00 8.88196601e-01]\n",
      " [1.00000000e+00 3.33066907e-16 1.00000000e+00 ... 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00]\n",
      " [8.58578644e-01 1.00000000e+00 1.11022302e-16 ... 1.00000000e+00\n",
      "  1.00000000e+00 9.20943058e-01]\n",
      " ...\n",
      " [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 2.22044605e-16\n",
      "  1.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 1.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00]\n",
      " [8.88196601e-01 1.00000000e+00 9.20943058e-01 ... 1.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(matrix)\n",
    "print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters , )\n",
    "\n",
    "km.fit(matrix)\n",
    "\n",
    "clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'tweets_cluster.pkl')\n",
    "\n",
    "\n",
    "\n",
    "km = joblib.load('tweets_cluster.pkl')\n",
    "clusters = km.labels_\n",
    "# print(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    3030\n",
       "3.0    2534\n",
       "1.0    2145\n",
       "2.0    1932\n",
       "0.0     113\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets= tweets.assign(cluster=pd.Series(km.labels_))\n",
    "tweets['cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " trump\n",
      " co\n",
      " business\n",
      " vaccine\n",
      "Cluster 1:\n",
      " business\n",
      " co\n",
      " small\n",
      " mind\n",
      "Cluster 2:\n",
      " health\n",
      " co\n",
      " mental\n",
      " care\n",
      "Cluster 3:\n",
      " sport\n",
      " people\n",
      " it\n",
      " one\n",
      "Cluster 4:\n",
      " co\n",
      " sport\n",
      " people\n",
      " it\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vec.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :4]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument has 13937 elements, which is inconsistent with 'x' and 'y' with size 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[0;32m   4238\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4239\u001b[1;33m                 \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4240\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;31m# Test dimensionality to reject single floats.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Invalid RGBA argument: {orig_c!r}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     \u001b[1;31m# Return a tuple to prevent the cached value from being modified.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid RGBA argument: 1.0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-b8cfa5fe474e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_kmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'viridis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcenters\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2809\u001b[0m         \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2810\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[1;32m-> 2811\u001b[1;33m     __ret = gca().scatter(\n\u001b[0m\u001b[0;32m   2812\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2813\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1563\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1565\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4400\u001b[0m         \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4401\u001b[1;33m             self._parse_scatter_color_args(\n\u001b[0m\u001b[0;32m   4402\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[0;32m   4240\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4241\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4242\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0minvalid_shape_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4243\u001b[0m                 \u001b[1;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4244\u001b[0m                 \u001b[1;31m# severe failure => one may appreciate a verbose feedback.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'c' argument has 13937 elements, which is inconsistent with 'x' and 'y' with size 1."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_kmeans = km.predict(matrix)\n",
    "plt.scatter(matrix[:, 0], matrix[:, 5], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers =km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 5], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'trump, co, business, vaccine',\n",
    "                 1: 'business, co, small,mind ' ,              \n",
    "                 2: 'health, co, mental, care', \n",
    "                 3: 'sport, people,it,one', \n",
    "                 4: 'co, sport, people,it'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.0, 15.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import statements\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# create blobs\n",
    "data = make_blobs(n_samples=200, n_features=2, centers=5, cluster_std=1.6, random_state=50)\n",
    "# create np array for data points\n",
    "points = data[0]\n",
    "# create scatter plot\n",
    "plt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B style=\"color:orangered;\">K-means With Jaccard Distance </B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><B>Clustering is a type of unsupervised machine learning which aims to find homogeneous subgroups such that objects in the same group (clusters) are more similar to each other than the others.</li>\n",
    "<li><B>KMeans is a clustering algorithm which divides observations into k clusters. Since we can dictate the amount of clusters, it can be easily used in classification where we divide data into clusters which can be equal to or more than the number of classes.</B></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re, string\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cachedStopWords = stopwords.words('english')\n",
    "\n",
    "class kMeans():\n",
    "    def __init__(self, seeds, tweets):\n",
    "        self.seeds = seeds\n",
    "        self.tweets = tweets\n",
    "        self.max_iterations = 1000\n",
    "        self.k = len(seeds)\n",
    "\n",
    "        self.clusters = {} # cluster to tweetID\n",
    "        self.rev_clusters = {} # reverse index, tweetID to cluster\n",
    "        self.jaccardMatrix = {} # stores pairwise jaccard distance in a matrix\n",
    "\n",
    "        self.initializeClusters()\n",
    "        self.initializeMatrix()\n",
    "\n",
    "    def jaccardDistance(self, setA, setB):\n",
    "        # Calcualtes the Jaccard Distance of two sets\n",
    "        try:\n",
    "            return 1 - float(len(setA.intersection(setB))) / float(len(setA.union(setB)))\n",
    "        except TypeError:\n",
    "            print ('Invalid type. Type set expected.')\n",
    "\n",
    "    def bagOfWords(self, string):\n",
    "        # Returns a bag of words from a given string\n",
    "        # Space delimited, removes punctuation, lowercase\n",
    "        # Cleans text from url, stop words, tweet @, and 'rt'\n",
    "        words = string.lower().strip().split(' ')\n",
    "        for word in words:\n",
    "            word = word.rstrip().lstrip()\n",
    "            if not re.match(r'^https?:\\/\\/.*[\\r\\n]*', word) \\\n",
    "            and not re.match('^@.*', word) \\\n",
    "            and not re.match('\\s', word) \\\n",
    "            and word not in cachedStopWords \\\n",
    "            and word != 'rt' \\\n",
    "            and word != '':\n",
    "                yield regex.sub('', word)\n",
    "\n",
    "    def initializeMatrix(self):\n",
    "        # Dynamic Programming: creates matrix storing pairwise jaccard distances\n",
    "        for ID1 in self.tweets:\n",
    "            self.jaccardMatrix[ID1] = {}\n",
    "            bag1 = set(self.bagOfWords(self.tweets[ID1]['text']))\n",
    "            for ID2 in self.tweets:\n",
    "                if ID2 not in self.jaccardMatrix:\n",
    "                    self.jaccardMatrix[ID2] = {}\n",
    "                bag2 = set(self.bagOfWords(self.tweets[ID2]['text']))\n",
    "                distance = self.jaccardDistance(bag1, bag2)\n",
    "                self.jaccardMatrix[ID1][ID2] = distance\n",
    "                self.jaccardMatrix[ID2][ID1] = distance\n",
    "\n",
    "    def initializeClusters(self):\n",
    "        # Initialize tweets to no cluster\n",
    "        for ID in self.tweets:\n",
    "            self.rev_clusters[ID] = -1\n",
    "\n",
    "        # Initialize clusters with seeds\n",
    "        for k in range(self.k):\n",
    "            self.clusters[k] = set([self.seeds[k]])\n",
    "            self.rev_clusters[self.seeds[k]] = k\n",
    "\n",
    "    def calcNewClusters(self):\n",
    "        # Initialize new cluster\n",
    "        new_clusters = {}\n",
    "        new_rev_cluster = {}\n",
    "        for k in range(self.k):\n",
    "            new_clusters[k] = set()\n",
    "\n",
    "        for ID in self.tweets:\n",
    "            min_dist = float(\"inf\")\n",
    "            min_cluster = self.rev_clusters[ID]\n",
    "\n",
    "            # Calculate min average distance to each cluster\n",
    "            for k in self.clusters:\n",
    "                dist = 0\n",
    "                count = 0\n",
    "                for ID2 in self.clusters[k]:\n",
    "                    dist += self.jaccardMatrix[ID][ID2]\n",
    "                    count += 1\n",
    "                if count > 0:\n",
    "                    avg_dist = dist/float(count)\n",
    "                    if min_dist > avg_dist:\n",
    "                        min_dist = avg_dist\n",
    "                        min_cluster = k\n",
    "            new_clusters[min_cluster].add(ID)\n",
    "            new_rev_cluster[ID] = min_cluster\n",
    "        return new_clusters, new_rev_cluster\n",
    "\n",
    "    def converge(self):\n",
    "        # Initialize previous cluster to compare changes with new clustering\n",
    "        new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "        self.clusters = copy.deepcopy(new_clusters)\n",
    "        self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "\n",
    "        # Converges until old and new iterations are the same\n",
    "        iterations = 1\n",
    "        while iterations < self.max_iterations:\n",
    "            new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "            iterations += 1\n",
    "            if self.rev_clusters != new_rev_clusters:\n",
    "                self.clusters = copy.deepcopy(new_clusters)\n",
    "                self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "            else:\n",
    "                #print iterations\n",
    "                return\n",
    "            \n",
    "    \n",
    "    def printClusterText(self):\n",
    "        # Prints text of clusters\n",
    "        for k in self.clusters:\n",
    "            for ID in self.clusters[k]:\n",
    "                print (self.tweets[ID]['text'])\n",
    "            print ('\\n')\n",
    " \n",
    "    def printClusters(self):\n",
    "        # Prints cluster ID and tweet IDs for that cluster\n",
    "        for k in self.clusters:\n",
    "            print (str(k) + ':' + ','.join(map(str,self.clusters[k])))\n",
    "\n",
    "    def printMatrix(self):\n",
    "        # Prints jaccard distance matrix\n",
    "        for ID in self.tweets:\n",
    "            for ID2 in self.tweets:\n",
    "                print (ID, ID2, self.jaccardMatrix[ID][ID2])\n",
    "\n",
    "\n",
    "\n",
    "    kmeans = kMeans(3, tweets)\n",
    "    kmeans.converge()\n",
    "    #kmeans.printClusterText()\n",
    "    kmeans.printClusters()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['lemma']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-be669ffc825d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mdata_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bdtweets1.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bdtweets1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'lemma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1935\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1936\u001b[0m             ):\n\u001b[1;32m-> 1937\u001b[1;33m                 \u001b[0m_validate_usecols_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1939\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[1;34m(usecols, names)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1232\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1233\u001b[0m             \u001b[1;34m\"Usecols do not match columns, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m             \u001b[1;34mf\"columns expected but not found: {missing}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['lemma']"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "\n",
    "def pre_process_tweets(url):\n",
    "\n",
    "    f = open(url, \"r\", encoding=\"utf8\")\n",
    "    tweets = list(f)\n",
    "    list_of_tweets = []\n",
    "\n",
    "    for i in range(len(tweets)):\n",
    "\n",
    "        # remove \\n from the end after every sentence\n",
    "        tweets[i] = tweets[i].strip('\\n')\n",
    "\n",
    "        # Remove the tweet id and timestamp\n",
    "        tweets[i] = tweets[i][50:]\n",
    "\n",
    "        # Remove any word that starts with the symbol @\n",
    "        tweets[i] = \" \".join(filter(lambda x: x[0] != '@', tweets[i].split()))\n",
    "\n",
    "        # Remove any URL\n",
    "        tweets[i] = re.sub(r\"http\\S+\", \"\", tweets[i])\n",
    "        tweets[i] = re.sub(r\"www\\S+\", \"\", tweets[i])\n",
    "\n",
    "        # remove colons from the end of the sentences (if any) after removing url\n",
    "        tweets[i] = tweets[i].strip()\n",
    "        tweet_len = len(tweets[i])\n",
    "        if tweet_len > 0:\n",
    "            if tweets[i][len(tweets[i]) - 1] == ':':\n",
    "                tweets[i] = tweets[i][:len(tweets[i]) - 1]\n",
    "\n",
    "        # Remove any hash-tags symbols\n",
    "        tweets[i] = tweets[i].replace('#', '')\n",
    "\n",
    "        # Convert every word to lowercase\n",
    "        tweets[i] = tweets[i].lower()\n",
    "\n",
    "        # remove punctuations\n",
    "        tweets[i] = tweets[i].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # trim extra spaces\n",
    "        tweets[i] = \" \".join(tweets[i].split())\n",
    "\n",
    "        # convert each tweet from string type to as list<string> using \" \" as a delimiter\n",
    "        list_of_tweets.append(tweets[i].split(' '))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return list_of_tweets\n",
    "\n",
    "\n",
    "def k_means(tweets, k=4, max_iterations=50):\n",
    "\n",
    "    centroids = []\n",
    "\n",
    "    # initialization, assign random tweets as centroids\n",
    "    count = 0\n",
    "    hash_map = dict()\n",
    "    while count < k:\n",
    "        random_tweet_idx = rd.randint(0, len(tweets) - 1)\n",
    "        if random_tweet_idx not in hash_map:\n",
    "            count += 1\n",
    "            hash_map[random_tweet_idx] = True\n",
    "            centroids.append(tweets[random_tweet_idx])\n",
    "\n",
    "    iter_count = 0\n",
    "    prev_centroids = []\n",
    "\n",
    "    # run the iterations until not converged or until the max iteration in not reached\n",
    "    while (is_converged(prev_centroids, centroids)) == False and (iter_count < max_iterations):\n",
    "\n",
    "        print(\"running iteration \" + str(iter_count))\n",
    "\n",
    "        # assignment, assign tweets to the closest centroids\n",
    "        clusters = assign_cluster(tweets, centroids)\n",
    "\n",
    "        # to check if k-means converges, keep track of prev_centroids\n",
    "        prev_centroids = centroids\n",
    "\n",
    "        # update, update centroid based on clusters formed\n",
    "        centroids = update_centroids(clusters)\n",
    "        iter_count = iter_count + 1\n",
    "\n",
    "    if (iter_count == max_iterations):\n",
    "        print(\"max iterations reached, K means not converged\")\n",
    "    else:\n",
    "        print(\"converged\")\n",
    "\n",
    "    sse = compute_SSE(clusters)\n",
    "\n",
    "    return clusters, sse\n",
    "\n",
    "\n",
    "def is_converged(prev_centroid, new_centroids):\n",
    "\n",
    "    # false if lengths are not equal\n",
    "    if len(prev_centroid) != len(new_centroids):\n",
    "        return False\n",
    "\n",
    "    # iterate over each entry of clusters and check if they are same\n",
    "    for c in range(len(new_centroids)):\n",
    "        if \" \".join(new_centroids[c]) != \" \".join(prev_centroid[c]):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def assign_cluster(tweets, centroids):\n",
    "\n",
    "    clusters = dict()\n",
    "\n",
    "    # for every tweet iterate each centroid and assign closest centroid to a it\n",
    "    for t in range(len(tweets)):\n",
    "        min_dis = math.inf\n",
    "        cluster_idx = -1;\n",
    "        for c in range(len(centroids)):\n",
    "            dis = getDistance(centroids[c], tweets[t])\n",
    "            # look for a closest centroid for a tweet\n",
    "\n",
    "            if centroids[c] == tweets[t]:\n",
    "                # print(\"tweet and centroid are equal with c: \" + str(c) + \", t\" + str(t))\n",
    "                cluster_idx = c\n",
    "                min_dis = 0\n",
    "                break\n",
    "\n",
    "            if dis < min_dis:\n",
    "                cluster_idx = c\n",
    "                min_dis = dis\n",
    "\n",
    "        # randomise the centroid assignment to a tweet if nothing is common\n",
    "        if min_dis == 1:\n",
    "            cluster_idx = rd.randint(0, len(centroids) - 1)\n",
    "\n",
    "        # assign the closest centroid to a tweet\n",
    "        clusters.setdefault(cluster_idx, []).append([tweets[t]])\n",
    "        # print(\"tweet t: \" + str(t) + \" is assigned to cluster c: \" + str(cluster_idx))\n",
    "        # add the tweet distance from its closest centroid to compute sse in the end\n",
    "        last_tweet_idx = len(clusters.setdefault(cluster_idx, [])) - 1\n",
    "        clusters.setdefault(cluster_idx, [])[last_tweet_idx].append(min_dis)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def update_centroids(clusters):\n",
    "\n",
    "    centroids = []\n",
    "\n",
    "    # iterate each cluster and check for a tweet with closest distance sum with all other tweets in the same cluster\n",
    "    # select that tweet as the centroid for the cluster\n",
    "    for c in range(len(clusters)):\n",
    "        min_dis_sum = math.inf\n",
    "        centroid_idx = -1\n",
    "\n",
    "        # to avoid redundant calculations\n",
    "        min_dis_dp = []\n",
    "\n",
    "        for t1 in range(len(clusters[c])):\n",
    "            min_dis_dp.append([])\n",
    "            dis_sum = 0\n",
    "            # get distances sum for every of tweet t1 with every tweet t2 in a same cluster\n",
    "            for t2 in range(len(clusters[c])):\n",
    "                if t1 != t2:\n",
    "                    if t2 < t1:\n",
    "                        dis = min_dis_dp[t2][t1]\n",
    "                    else:\n",
    "                        dis = getDistance(clusters[c][t1][0], clusters[c][t2][0])\n",
    "\n",
    "                    min_dis_dp[t1].append(dis)\n",
    "                    dis_sum += dis\n",
    "                else:\n",
    "                    min_dis_dp[t1].append(0)\n",
    "\n",
    "            # select the tweet with the minimum distances sum as the centroid for the cluster\n",
    "            if dis_sum < min_dis_sum:\n",
    "                min_dis_sum = dis_sum\n",
    "                centroid_idx = t1\n",
    "\n",
    "        # append the selected tweet to the centroid list\n",
    "        centroids.append(clusters[c][centroid_idx][0])\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def getDistance(tweet1, tweet2):\n",
    "\n",
    "    # get the intersection\n",
    "    intersection = set(tweet1).intersection(tweet2)\n",
    "\n",
    "    # get the union\n",
    "    union = set().union(tweet1, tweet2)\n",
    "\n",
    "    # return the jaccard distance\n",
    "    return 1 - (len(intersection) / len(union))\n",
    "\n",
    "\n",
    "def compute_SSE(clusters):\n",
    "\n",
    "    sse = 0\n",
    "    # iterate every cluster 'c', compute SSE as the sum of square of distances of the tweet from it's centroid\n",
    "    for c in range(len(clusters)):\n",
    "        for t in range(len(clusters[c])):\n",
    "            sse = sse + (clusters[c][t][1] * clusters[c][t][1])\n",
    "\n",
    "    return sse\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_url = 'bdtweets1.csv'\n",
    "\n",
    "    tweets = tweets = pd.read_csv(\"bdtweets1.csv\", usecols = ['lemma'])\n",
    "\n",
    "\n",
    "    # default number of experiments to be performed\n",
    "    experiments = 2\n",
    "\n",
    "    # default value of K for K-means\n",
    "    k = 3\n",
    "\n",
    "    # for every experiment 'e', run K-means\n",
    "    for e in range(experiments):\n",
    "\n",
    "        print(\"------ Running K means for experiment no. \" + str((e + 1)) + \" for k = \" + str(k))\n",
    "\n",
    "        clusters, sse = k_means(tweets, k)\n",
    "\n",
    "        # for every cluster 'c', print size of each cluster\n",
    "        for c in range(len(clusters)):\n",
    "            print(str(c+1) + \": \", str(len(clusters[c])) + \" tweets\")\n",
    "             # to print tweets in a cluster\n",
    "            for t in range(len(clusters[c])):\n",
    "                print(\"t\" + str(t) + \", \" + (\" \".join(clusters[c][t][0])))\n",
    "\n",
    "        print(\"--> SSE : \" + str(sse))\n",
    "        print('\\n')\n",
    "\n",
    "         #increment k after every experiment\n",
    "        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "class Jaccard:\n",
    "\n",
    "\tdef  __init__(self,args,args_ii):\n",
    "\t\tself.args = self.tokenize(args)\n",
    "\t\tself.args_ii = self.tokenize(args_ii)\n",
    "\n",
    "\tdef intersect(self):\n",
    "\t\treturn [i for i in self.args if i in self.args_ii]\n",
    "\n",
    "\tdef union(self):\n",
    "\t\treturn set(self.args + self.args_ii)\n",
    "\n",
    "\tdef similarity(self):\n",
    "\t\tintersect = len(self.intersect())\n",
    "\t\tunion = len(self.union())\n",
    "\t\tif intersect == None and union == None:\n",
    "\t\t\treturn 1\n",
    "\t\treturn intersect / union\n",
    "\n",
    "\tdef distance(self):\n",
    "\t\treturn 1 - self.similarity()\n",
    "\n",
    "\tdef tokenize(self,item):\n",
    "\t\titem = item.lower()\n",
    "\t\treturn item.split(\" \")\n",
    "\n",
    "jaccard = Jaccard(\"tweets[text][88]\",\"tweets[text][12520]\")\n",
    "\n",
    "similarity = jaccard.similarity()\n",
    "print (similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : Please enter files in this format <json file> <InitialSeeds file> <outputfile.txt>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'live_tweets1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-a5f45ee44a44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;31m# initialize code and load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m \u001b[0mTC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweetClusering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlive_tweets1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;31m# Create clusters based on initial seeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[0mTC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCreate_Clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'live_tweets1' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import re, string\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "StopWord = stopwords.words('english')\n",
    "\n",
    "\n",
    "class TweetClusering():\n",
    "    def __init__(self, JsoneFile, InitialSeedsFile,iterations):\n",
    "\n",
    "        tweets_ID = {}\n",
    "        with open(JsoneFile, 'r') as f:\n",
    "            for line in f:\n",
    "                tweet = json.loads(line)\n",
    "                tweets_ID[tweet['id']] = tweet\n",
    "\n",
    "        f = open(InitialSeedsFile)\n",
    "        InitialSeeds = [int(line.rstrip(',\\n')) for line in f.readlines()]\n",
    "        f.close()\n",
    "\n",
    "        self.InitialSeeds = InitialSeeds\n",
    "        self.tweets_ID = tweets_ID\n",
    "        self.max_iterations = iterations\n",
    "        self.k = len(InitialSeeds)\n",
    "        self.clusters = {}  # cluster to tweetID\n",
    "        self.rev_clusters = {}  # reverse index, tweetID to cluster\n",
    "        self.JC_MAT = {}  # stores pairwise jaccard distance in a matrix\n",
    "\n",
    "    def jaccardDistance(self, setA, setB):\n",
    "        # Calcualtes the Jaccard Distance of two sets\n",
    "        return 1 - float(len(setA.intersection(setB))) / float(len(setA.union(setB)))\n",
    "\n",
    "    def Dictionary(self, string):\n",
    "        # Returns a bag of words from a given string\n",
    "        # Space delimited, removes punctuation, lowercase\n",
    "        # Cleans text from url, stop words, tweet @, and 'rt'\n",
    "        words = string.lower().strip().split(' ')\n",
    "        for word in words:\n",
    "            word = word.rstrip().lstrip()\n",
    "            if not re.match(r'^https?:\\/\\/.*[\\r\\n]*', word) \\\n",
    "                    and not re.match('^@.*', word) \\\n",
    "                    and not re.match('\\s', word) \\\n",
    "                    and word not in StopWord \\\n",
    "                    and word != 'rt' \\\n",
    "                    and word != '':\n",
    "                yield regex.sub('', word)\n",
    "\n",
    "    def Create_Matrix(self):\n",
    "        # Dynamic Programming: creates matrix storing pairwise jaccard distances\n",
    "        for ID1 in self.tweets_ID:\n",
    "            self.JC_MAT[ID1] = {}\n",
    "            TW1 = set(self.Dictionary(self.tweets_ID[ID1]['text']))\n",
    "            for ID2 in self.tweets_ID:\n",
    "                if ID2 not in self.JC_MAT:\n",
    "                    self.JC_MAT[ID2] = {}\n",
    "                TW2 = set(self.Dictionary(self.tweets_ID[ID2]['text']))\n",
    "                distance = self.jaccardDistance(TW1, TW2)\n",
    "                self.JC_MAT[ID1][ID2] = distance\n",
    "                self.JC_MAT[ID2][ID1] = distance\n",
    "\n",
    "    def Create_Clusters(self):\n",
    "        # Initialize tweets_ID to no cluster\n",
    "        for ID in self.tweets_ID:\n",
    "            self.rev_clusters[ID] = -1\n",
    "\n",
    "        # Initialize clusters with InitialSeeds\n",
    "        for k in range(self.k):\n",
    "            self.clusters[k] = set([self.InitialSeeds[k]])\n",
    "            self.rev_clusters[self.InitialSeeds[k]] = k\n",
    "\n",
    "    def calcNewClusters(self):\n",
    "        # Initialize new cluster\n",
    "        new_clusters = {}\n",
    "        new_rev_cluster = {}\n",
    "        for k in range(self.k):\n",
    "            new_clusters[k] = set()\n",
    "\n",
    "        for ID in self.tweets_ID:\n",
    "            min_dist = float(\"inf\")\n",
    "            min_cluster = self.rev_clusters[ID]\n",
    "\n",
    "            # Calculate min average distance to each cluster\n",
    "            for k in self.clusters:\n",
    "                dist = 0\n",
    "                count = 0\n",
    "                for ID2 in self.clusters[k]:\n",
    "                    dist += self.JC_MAT[ID][ID2]\n",
    "                    count += 1\n",
    "                if count > 0:\n",
    "                    avg_dist = dist / float(count)\n",
    "                    if min_dist > avg_dist:\n",
    "                        min_dist = avg_dist\n",
    "                        min_cluster = k\n",
    "            new_clusters[min_cluster].add(ID)\n",
    "            new_rev_cluster[ID] = min_cluster\n",
    "        return new_clusters, new_rev_cluster\n",
    "\n",
    "    def optimize(self):\n",
    "        # Initialize previous cluster to compare changes with new clustering\n",
    "        new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "        self.clusters = copy.deepcopy(new_clusters)\n",
    "        self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "\n",
    "        # Converges until old and new iterations are the same\n",
    "        iterations = 1\n",
    "        while iterations < self.max_iterations:\n",
    "            new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "            iterations += 1\n",
    "            if self.rev_clusters != new_rev_clusters:\n",
    "                self.clusters = copy.deepcopy(new_clusters)\n",
    "                self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    def calc_SSE(self):\n",
    "        self.SSE = []\n",
    "        for cluster in self.clusters:\n",
    "            error = 0\n",
    "            for ID1 in self.clusters[cluster]:\n",
    "                error += (self.JC_MAT[self.InitialSeeds[cluster]][ID1])\n",
    "            self.SSE.append(error)\n",
    "\n",
    "    def printClusters(self,out_file):\n",
    "        # Prints cluster ID and tweet IDs for that cluster\n",
    "        file = open(out_file, 'w+')\n",
    "        file.write('CS6375.004 - Machine Learning: Assignment-6 : Part II - Tweets Clustering using k-means \\n\\n')\n",
    "        K_clusters=[]\n",
    "        for k in self.clusters:\n",
    "            K_clusters.append(str(k+1) + ':' + ','.join(map(str, self.clusters[k])))\n",
    "            #print(str(k+1) + ' : ' + ','.join(map(str, self.clusters[k])))\n",
    "            file.write('Cluster '+ str(k+1) + ' : ' + ', '.join(map(str, self.clusters[k])))\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        '''\n",
    "        print(\"======================================================\")\n",
    "        for cluster in self.clusters:\n",
    "            print('For cluster '+str(cluster+1) + ' : SSE = ' + str(round(self.SSE[cluster], 3)))\n",
    "        '''\n",
    "        TotalSSE = sum(self.SSE)\n",
    "        print(\"======================================================\")\n",
    "        print(\"Validation SSE = %3.4f\"%TotalSSE)\n",
    "        print('\\nResults are available in output file')\n",
    "        file.write(\"======================================================\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"Number of Clusters = %d\" % len(self.SSE))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"Validation SSE = %3.4f\" % TotalSSE)\n",
    "\n",
    "# the main part starts from here\n",
    "if len(sys.argv) != 4:\n",
    "    print(\"Error : Please enter files in this format <json file> <InitialSeeds file> <outputfile.txt>\")\n",
    "    exit(-1)\n",
    "# initialize code and load data\n",
    "TC = TweetClusering(live_tweets1.json, sys.argv[2],1000)\n",
    "# Create clusters based on initial seeds\n",
    "TC.Create_Clusters()\n",
    "# create distance matrix using Jaccard distance\n",
    "TC.Create_Matrix()\n",
    "# optimize to find the best clusters\n",
    "TC.optimize()\n",
    "# calculate SSE\n",
    "TC.calc_SSE()\n",
    "# show and save results in the given text files\n",
    "TC.printClusters(sys.argv[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,sys\n",
    "\n",
    "# Reading json file in a dictionary\n",
    "def getTweets(tweetsJsonFile):\n",
    "    tweets = {}\n",
    "    with open(tweetsJsonFile) as json_data:\n",
    "        for line in json_data:\n",
    "            tweet = json.loads(line)\n",
    "            tweets[str(tweet[\"id\"])] = tweet[\"text\"]\n",
    "            print(tweets)\n",
    "    return tweets\n",
    "test= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    "def jaccard_similarity(text,text1):\n",
    "    intersection_cardinality = len(set.intersection(*[set(text1), set(text2)]))\n",
    "    union_cardinality = len(set.union(*[set(text1), set(text2)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05263157894736842"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(tweets['lemma'][1],tweets['lemma'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(tweets['lemma'][1][0],tweets['lemma'][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'health'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['lemma'][1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'servic'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['lemma'][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collection import counter\n",
    "\n",
    "def intersection(tweetdata_one, tweetdata_two): #function count intersection between two words\n",
    "    result_intesection = 0\n",
    "    for word in tweetdata_one:\n",
    "        while tweetdata_one[word] != 0 and word in tweetdata_two:\n",
    "            if word in tweetdata_two:\n",
    "                tweetdata_two[word] = tweetdata_two[word] - 1\n",
    "                tweetdata_one[word] = tweetdata_one[word] - 1\n",
    "                if tweetdata_two[word] == 0:\n",
    "                    tweetdata_two.pop(word, None)\n",
    "                result_intesection += 1\n",
    "    return result_intesection\n",
    "\n",
    "def union(tweetdata_one, tweetdata_two): #function count union of two words\n",
    "    result_union = 0\n",
    "    for word in tweetdata_one:\n",
    "        if word in tweetdata_two:\n",
    "            result_union = result_union + max(tweetdata_one[word], tweetdata_two[word])\n",
    "            tweetdata_two.pop(word, None)\n",
    "        else:\n",
    "            result_union = result_union + tweetdata_one[word]\n",
    "    for word in tweetdata_two:\n",
    "        result_union = result_union + tweetdata_two[word]\n",
    "    return result_union\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jaccard_distance(tweetDataOne, tweetDataTwo):\n",
    "    tweetDataOne_count = countWords(tweetDataOne)\n",
    "    tweetDataTwo_count = countWords(tweetDataTwo)\n",
    "    tweetdata_union = union(dict(tweetDataOne_count), dict(tweetDataTwo_count))\n",
    "    tweetdata_intersect = intersection(dict(tweetDataOne_count), dict(tweetDataTwo_count))\n",
    "    return 1.0 - tweetdata_intersect * 1.0 / tweetdata_union\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-836d4eef6272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjaccard_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-c4e1badd16cf>\u001b[0m in \u001b[0;36mjaccard_distance\u001b[1;34m(tweetDataOne, tweetDataTwo)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjaccard_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetDataOne\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweetDataTwo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mtweetDataOne_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetDataOne\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mtweetDataTwo_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetDataTwo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mtweetdata_union\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetDataOne_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetDataTwo_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'countWords' is not defined"
     ]
    }
   ],
   "source": [
    "jaccard_distance(tweets['lemma'][0],tweets['lemma'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " def jaccardDistance(setA, setB):\n",
    "        # Calcualtes the Jaccard Distance of two sets\n",
    "        try:\n",
    "            return 1 - float(len(setA.intersection(setB))) / float(len(setA.union(setB)))\n",
    "        except TypeError:\n",
    "            print ('Invalid type. Type set expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "jaccardDistance(set(tweets['lemma'][0]), set(tweets['lemma'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'anoth', 'go', 'busi', 'sale']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['lemma'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-6a7ea690cc57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# learning the clustering from the input date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_k_means.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;31m# avoid forcing order when copy_x=False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0m\u001b[0;32m    858\u001b[0m                         order=order, copy=self.copy_x)\n\u001b[0;32m    859\u001b[0m         \u001b[1;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    513\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "# read csv input file\n",
    "# input_data = pd.read_csv(\"input_data.txt\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "# initialize KMeans object specifying the number of desired clusters\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "\n",
    "# learning the clustering from the input date\n",
    "kmeans.fit(tweets['lemma'].values)\n",
    "\n",
    "\n",
    "# output the labels for the input data\n",
    "print(kmeans.labels_)\n",
    "\n",
    "\n",
    "# predict the classification for given data sample \n",
    "# predicted_class = kmeans.predict([[1, 10, 15]])\n",
    "# print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846341176535043\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(444447777)\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import json\n",
    "import re, string\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cachedStopWords = stopwords.words('english')\n",
    "\n",
    "class kMeans():\n",
    "    def __init__(self, seeds, tweets):\n",
    "        self.seeds = seeds\n",
    "        self.tweets = tweets\n",
    "        self.max_iterations = 10\n",
    "        self.k = len(seeds)\n",
    "\n",
    "        self.clusters = {} # cluster to tweetID\n",
    "        self.rev_clusters = {} # reverse index, tweetID to cluster\n",
    "        self.jaccardMatrix = {} # stores pairwise jaccard distance in a matrix\n",
    "\n",
    "        self.initializeClusters()\n",
    "        self.initializeMatrix()\n",
    "\n",
    "    def jaccardDistance(self, setA, setB):\n",
    "        # Calcualtes the Jaccard Distance of two sets\n",
    "        try:\n",
    "            return 1 - float(len(setA.intersection(setB))) / float(len(setA.union(setB)))\n",
    "        except TypeError:\n",
    "            print 'Invalid type. Type set expected.'\n",
    "\n",
    "    def bagOfWords(self, string):\n",
    "        # Returns a bag of words from a given string\n",
    "        # Space delimited, removes punctuation, lowercase\n",
    "        # Cleans text from url, stop words, tweet @, and 'rt'\n",
    "        words = string.lower().strip().split(' ')\n",
    "        for word in words:\n",
    "            word = word.rstrip().lstrip()\n",
    "            if not re.match(r'^https?:\\/\\/.*[\\r\\n]*', word) \\\n",
    "            and not re.match('^@.*', word) \\\n",
    "            and not re.match('\\s', word) \\\n",
    "            and word not in cachedStopWords \\\n",
    "            and word != 'rt' \\\n",
    "            and word != '':\n",
    "                yield regex.sub('', word)\n",
    "\n",
    "    def initializeMatrix(self):\n",
    "        # Dynamic Programming: creates matrix storing pairwise jaccard distances\n",
    "        for ID1 in self.tweets:\n",
    "            self.jaccardMatrix[ID1] = {}\n",
    "            bag1 = set(self.bagOfWords(self.tweets[ID1]['text']))\n",
    "            for ID2 in self.tweets:\n",
    "                if ID2 not in self.jaccardMatrix:\n",
    "                    self.jaccardMatrix[ID2] = {}\n",
    "                bag2 = set(self.bagOfWords(self.tweets[ID2]['text']))\n",
    "                distance = self.jaccardDistance(bag1, bag2)\n",
    "                self.jaccardMatrix[ID1][ID2] = distance\n",
    "                self.jaccardMatrix[ID2][ID1] = distance\n",
    "\n",
    "    def initializeClusters(self):\n",
    "        # Initialize tweets to no cluster\n",
    "        for ID in self.tweets:\n",
    "            self.rev_clusters[ID] = -1\n",
    "\n",
    "        # Initialize clusters with seeds\n",
    "        for k in range(self.k):\n",
    "            self.clusters[k] = set([self.seeds[k]])\n",
    "            self.rev_clusters[self.seeds[k]] = k\n",
    "\n",
    "    def calcNewClusters(self):\n",
    "        # Initialize new cluster\n",
    "        new_clusters = {}\n",
    "        new_rev_cluster = {}\n",
    "        for k in range(self.k):\n",
    "            new_clusters[k] = set()\n",
    "\n",
    "        for ID in self.tweets:\n",
    "            min_dist = float(\"inf\")\n",
    "            min_cluster = self.rev_clusters[ID]\n",
    "\n",
    "            # Calculate min average distance to each cluster\n",
    "            for k in self.clusters:\n",
    "                dist = 0\n",
    "                count = 0\n",
    "                for ID2 in self.clusters[k]:\n",
    "                    dist += self.jaccardMatrix[ID][ID2]\n",
    "                    count += 1\n",
    "                if count > 0:\n",
    "                    avg_dist = dist/float(count)\n",
    "                    if min_dist > avg_dist:\n",
    "                        min_dist = avg_dist\n",
    "                        min_cluster = k\n",
    "            new_clusters[min_cluster].add(ID)\n",
    "            new_rev_cluster[ID] = min_cluster\n",
    "        return new_clusters, new_rev_cluster\n",
    "\n",
    "    def converge(self):\n",
    "        # Initialize previous cluster to compare changes with new clustering\n",
    "        new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "        self.clusters = copy.deepcopy(new_clusters)\n",
    "        self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "\n",
    "        # Converges until old and new iterations are the same\n",
    "        iterations = 1\n",
    "        while iterations < self.max_iterations:\n",
    "            new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "            iterations += 1\n",
    "            if self.rev_clusters != new_rev_clusters:\n",
    "                self.clusters = copy.deepcopy(new_clusters)\n",
    "                self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "            else:\n",
    "                #print iterations\n",
    "                return\n",
    "            \n",
    "    \n",
    "    def printClusterText(self):\n",
    "        # Prints text of clusters\n",
    "        for k in self.clusters:\n",
    "            for ID in self.clusters[k]:\n",
    "                print self.tweets[ID]['text']\n",
    "            print '\\n'\n",
    " \n",
    "    def printClusters(self):\n",
    "        # Prints cluster ID and tweet IDs for that cluster\n",
    "        for k in self.clusters:\n",
    "            print str(k) + ':' + ','.join(map(str,self.clusters[k]))\n",
    "\n",
    "    def printMatrix(self):\n",
    "        # Prints jaccard distance matrix\n",
    "        for ID in self.tweets:\n",
    "            for ID2 in self.tweets:\n",
    "                print ID, ID2, self.jaccardMatrix[ID][ID2]\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print >> sys.stderr, 'Usage: %s [json file] [seeds file]' % (sys.argv[0])\n",
    "        exit(-1)\n",
    "    \n",
    "    tweets = {}\n",
    "    with open(sys.argv[1], 'r') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tweets[tweet['id']] = tweet\n",
    "    \n",
    "    f = open(sys.argv[2])\n",
    "    seeds = [int(line.rstrip(',\\n')) for line in f.readlines()]\n",
    "    f.close()\n",
    "\n",
    "    kmeans = kMeans(seeds, tweets)\n",
    "    kmeans.converge()\n",
    "    #kmeans.printClusterText()\n",
    "    kmeans.printClusters()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re, string\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cachedStopWords = stopwords.words('english')\n",
    "\n",
    "class kMeans():\n",
    "    def __init__(self, seeds, tweets):\n",
    "        self.seeds = seeds\n",
    "        self.tweets = tweets\n",
    "        self.max_iterations = 10\n",
    "        self.k = len(seeds)\n",
    "\n",
    "        self.clusters = {} # cluster to tweetID\n",
    "        self.rev_clusters = {} # reverse index, tweetID to cluster\n",
    "        self.jaccardMatrix = {} # stores pairwise jaccard distance in a matrix\n",
    "\n",
    "        self.initializeClusters()\n",
    "        self.initializeMatrix()\n",
    "\n",
    "    \n",
    "    def intersection(tweetdata_one, tweetdata_two): #function count intersection between two words\n",
    "    result_intesection = 0\n",
    "    for word in tweetdata_one:\n",
    "        while tweetdata_one[word] != 0 and word in tweetdata_two:\n",
    "            if word in tweetdata_two:\n",
    "                tweetdata_two[word] = tweetdata_two[word] - 1\n",
    "                tweetdata_one[word] = tweetdata_one[word] - 1\n",
    "                if tweetdata_two[word] == 0:\n",
    "                    tweetdata_two.pop(word, None)\n",
    "                result_intesection += 1\n",
    "    return result_intesection\n",
    "\n",
    "    def union(tweetdata_one, tweetdata_two): #function count union of two words\n",
    "    result_union = 0\n",
    "    for word in tweetdata_one:\n",
    "        if word in tweetdata_two:\n",
    "            result_union = result_union + max(tweetdata_one[word], tweetdata_two[word])\n",
    "            tweetdata_two.pop(word, None)\n",
    "        else:\n",
    "            result_union = result_union + tweetdata_one[word]\n",
    "    for word in tweetdata_two:\n",
    "        result_union = result_union + tweetdata_two[word]\n",
    "    return result_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re, string\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "cachedStopWords = stopwords.words('english')\n",
    "\n",
    "class kMeans():\n",
    "    def __init__(self, seeds, tweets):\n",
    "        self.seeds = seeds\n",
    "        self.tweets = tweets\n",
    "        self.max_iterations = 30\n",
    "        self.k = len(seeds)\n",
    "\n",
    "        self.clusters = {} # cluster for tweet text\n",
    "        self.rev_clusters = {} # reverse index, tweetID to cluster\n",
    "        self.jaccardMatrix = {} # stores pairwise jaccard distance in a matrix\n",
    "\n",
    "        self.initializeClusters()\n",
    "        self.initializeMatrix()\n",
    "\n",
    "    def jaccardDistance(self, setA, setB):\n",
    "        # Calcualtes the Jaccard Distance of two sets\n",
    "        try:\n",
    "            return 1 - float(len(setA.intersection(setB))) / float(len(setA.union(setB)))\n",
    "        except TypeError:\n",
    "            print ('Invalid type. Type set expected.')\n",
    "\n",
    "    def bagOfWords(self, string):\n",
    "        # Returns a bag of words from a given string\n",
    "        # Space delimited, removes punctuation, lowercase\n",
    "        # Cleans text from url, stop words, tweet @, and 'rt'\n",
    "        words = string.lower().strip().split(' ')\n",
    "        for word in words:\n",
    "            word = word.rstrip().lstrip()\n",
    "            if not re.match(r'^https?:\\/\\/.*[\\r\\n]*', word) \\\n",
    "            and not re.match('^@.*', word) \\\n",
    "            and not re.match('\\s', word) \\\n",
    "            and word not in cachedStopWords \\\n",
    "            and word != 'rt' \\\n",
    "            and word != '':\n",
    "                yield regex.sub('', word)\n",
    "\n",
    "    def initializeMatrix(self):\n",
    "        # Dynamic Programming: creates matrix storing pairwise jaccard distances\n",
    "        for text1 in self.tweets:\n",
    "            self.jaccardMatrix[text1] = {}\n",
    "            bag1 = set(self.bagOfWords(self.tweets[tex1]['text']))\n",
    "            for ID2 in self.tweets:\n",
    "                if ID2 not in self.jaccardMatrix:\n",
    "                    self.jaccardMatrix[ID2] = {}\n",
    "                bag2 = set(self.bagOfWords(self.tweets[ID2]['text']))\n",
    "                distance = self.jaccardDistance(bag1, bag2)\n",
    "                self.jaccardMatrix[ID1][ID2] = distance\n",
    "                self.jaccardMatrix[ID2][ID1] = distance\n",
    "\n",
    "    def initializeClusters(self):\n",
    "        # Initialize tweets to no cluster\n",
    "        for ID in self.tweets:\n",
    "            self.rev_clusters[ID] = -1\n",
    "\n",
    "        # Initialize clusters with seeds\n",
    "        for k in range(self.k):\n",
    "            self.clusters[k] = set([self.seeds[k]])\n",
    "            self.rev_clusters[self.seeds[k]] = k\n",
    "\n",
    "    def calcNewClusters(self):\n",
    "        # Initialize new cluster\n",
    "        new_clusters = {}\n",
    "        new_rev_cluster = {}\n",
    "        for k in range(self.k):\n",
    "            new_clusters[k] = set()\n",
    "\n",
    "        for ID in self.tweets:\n",
    "            min_dist = float(\"inf\")\n",
    "            min_cluster = self.rev_clusters[ID]\n",
    "\n",
    "            # Calculate min average distance to each cluster\n",
    "            for k in self.clusters:\n",
    "                dist = 0\n",
    "                count = 0\n",
    "                for ID2 in self.clusters[k]:\n",
    "                    dist += self.jaccardMatrix[ID][ID2]\n",
    "                    count += 1\n",
    "                if count > 0:\n",
    "                    avg_dist = dist/float(count)\n",
    "                    if min_dist > avg_dist:\n",
    "                        min_dist = avg_dist\n",
    "                        min_cluster = k\n",
    "            new_clusters[min_cluster].add(ID)\n",
    "            new_rev_cluster[ID] = min_cluster\n",
    "        return new_clusters, new_rev_cluster\n",
    "\n",
    "    def converge(self):\n",
    "        # Initialize previous cluster to compare changes with new clustering\n",
    "        new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "        self.clusters = copy.deepcopy(new_clusters)\n",
    "        self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "\n",
    "        # Converges until old and new iterations are the same\n",
    "        iterations = 1\n",
    "        while iterations < self.max_iterations:\n",
    "            new_clusters, new_rev_clusters = self.calcNewClusters()\n",
    "            iterations += 1\n",
    "            if self.rev_clusters != new_rev_clusters:\n",
    "                self.clusters = copy.deepcopy(new_clusters)\n",
    "                self.rev_clusters = copy.deepcopy(new_rev_clusters)\n",
    "            else:\n",
    "                #print iterations\n",
    "                return\n",
    "            \n",
    "    \n",
    "    def printClusterText(self):\n",
    "        # Prints text of clusters\n",
    "        for k in self.clusters:\n",
    "            for ID in self.clusters[k]:\n",
    "                print self.tweets[ID]['text']\n",
    "            print '\\n'\n",
    " \n",
    "    def printClusters(self):\n",
    "        # Prints cluster ID and tweet IDs for that cluster\n",
    "        for k in self.clusters:\n",
    "            print str(k) + ':' + ','.join(map(str,self.clusters[k]))\n",
    "\n",
    "    def printMatrix(self):\n",
    "        # Prints jaccard distance matrix\n",
    "        for ID in self.tweets:\n",
    "            for ID2 in self.tweets:\n",
    "                print ID, ID2, self.jaccardMatrix[ID][ID2]\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print >> sys.stderr, 'Usage: %s [json file] [seeds file]' % (sys.argv[0])\n",
    "        exit(-1)\n",
    "    \n",
    "    tweets = {}\n",
    "    with open(sys.argv[1], 'r') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tweets[tweet['id']] = tweet\n",
    "    \n",
    "    f = open(sys.argv[2])\n",
    "    seeds = [int(line.rstrip(',\\n')) for line in f.readlines()]\n",
    "    f.close()\n",
    "\n",
    "    kmeans = kMeans(seeds, tweets)\n",
    "    kmeans.converge()\n",
    "    kmeans.printClusterText()\n",
    "    kmeans.printClusters()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "w1 = 'mappingsttr'\n",
    "w2 = 'mappings'\n",
    "\n",
    "nltk.edit_distance(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "vect = TfidfVectorizer(tokenizer=identity_tokenizer,lowercase=False)\n",
    "x = vect.fit_transform(tweets.lemma.values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ⓗⓐ',\n",
       " 'networ',\n",
       " 'nerdcant',\n",
       " 'nervewrack',\n",
       " 'nestl',\n",
       " 'netmatico',\n",
       " 'netting.diego',\n",
       " 'netwo',\n",
       " 'networkservic',\n",
       " 'neutral',\n",
       " 'neurip',\n",
       " 'neuro',\n",
       " 'neurologist',\n",
       " 'neurosci',\n",
       " 'neurosurgeo',\n",
       " 'neurotox',\n",
       " 'nerd',\n",
       " 'neptun',\n",
       " \"nephew'\",\n",
       " 'nepal',\n",
       " 'neonazi',\n",
       " 'neonat',\n",
       " 'neon',\n",
       " 'neomarx',\n",
       " 'neog',\n",
       " 'neo',\n",
       " 'nelson',\n",
       " 'nellen',\n",
       " 'neill',\n",
       " 'neilallen',\n",
       " 'neighbour',\n",
       " \"negvac'\",\n",
       " 'negati',\n",
       " 'neurotoxin',\n",
       " 'nevi',\n",
       " 'neek',\n",
       " 'ngam',\n",
       " 'nexigo',\n",
       " 'nextgen',\n",
       " 'nextlab',\n",
       " 'nexu',\n",
       " 'nffn',\n",
       " 'nfu',\n",
       " 'ngl',\n",
       " 'newark',\n",
       " 'ngold',\n",
       " 'ngonian',\n",
       " 'nhgopcoronacaucu',\n",
       " 'nhifdoctorshealth',\n",
       " 'nhl']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(vect.idf_)[::-1]\n",
    "features = vect.get_feature_names()\n",
    "top_n = 50\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=20, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " one\n",
      " /\n",
      " .\n",
      " …\n",
      " busi\n",
      " health\n",
      " love\n",
      " ...\n",
      " best\n",
      " ’\n",
      "Cluster 1:\n",
      " like\n",
      " /\n",
      " .\n",
      " …\n",
      " busi\n",
      " look\n",
      " health\n",
      " ’\n",
      " get\n",
      " i\n",
      "Cluster 2:\n",
      " ’\n",
      " s\n",
      " it\n",
      " /\n",
      " .\n",
      " i\n",
      " …\n",
      " m\n",
      " busi\n",
      " that\n",
      "Cluster 3:\n",
      " )\n",
      " (\n",
      " /\n",
      " …\n",
      " busi\n",
      " .\n",
      " health\n",
      " not\n",
      " free\n",
      " dm\n",
      "Cluster 4:\n",
      " mental\n",
      " health\n",
      " /\n",
      " .\n",
      " …\n",
      " ’\n",
      " take\n",
      " physic\n",
      " good\n",
      " care\n",
      "Cluster 5:\n",
      " health\n",
      " /\n",
      " …\n",
      " care\n",
      " .\n",
      " public\n",
      " good\n",
      " depart\n",
      " team\n",
      " biden\n",
      "Cluster 6:\n",
      " \"\n",
      " /\n",
      " …\n",
      " .\n",
      " busi\n",
      " health\n",
      " the\n",
      " say\n",
      " ?\n",
      " like\n",
      "Cluster 7:\n",
      " '\n",
      " /\n",
      " …\n",
      " .\n",
      " s\n",
      " health\n",
      " busi\n",
      " heir\n",
      " rover\n",
      " opt\n",
      "Cluster 8:\n",
      " /\n",
      " …\n",
      " busi\n",
      " .\n",
      " sport\n",
      " get\n",
      " make\n",
      " peopl\n",
      " new\n",
      " us\n",
      "Cluster 9:\n",
      " busi\n",
      " …\n",
      " sport\n",
      " .\n",
      " get\n",
      " love\n",
      " go\n",
      " great\n",
      " good\n",
      " need\n",
      "Cluster 10:\n",
      " t\n",
      " ’\n",
      " don\n",
      " /\n",
      " .\n",
      " busi\n",
      " can\n",
      " …\n",
      " y\n",
      " all\n",
      "Cluster 11:\n",
      " say\n",
      " /\n",
      " …\n",
      " .\n",
      " busi\n",
      " health\n",
      " ’\n",
      " “\n",
      " sport\n",
      " peopl\n",
      "Cluster 12:\n",
      " small\n",
      " busi\n",
      " /\n",
      " own\n",
      " black\n",
      " …\n",
      " owner\n",
      " .\n",
      " t.co/mo5r6ckzu5\n",
      " hey\n",
      "Cluster 13:\n",
      " vaccin\n",
      " /\n",
      " covid\n",
      " 19\n",
      " …\n",
      " pfizer\n",
      " health\n",
      " .\n",
      " get\n",
      " first\n",
      "Cluster 14:\n",
      " psg\n",
      " istanbul\n",
      " basaksehir\n",
      " walk\n",
      " /\n",
      " player\n",
      " match\n",
      " racism\n",
      " pitch\n",
      " suspend\n",
      "Cluster 15:\n",
      " mind\n",
      " busi\n",
      " .\n",
      " /\n",
      " ’\n",
      " ya\n",
      " ur\n",
      " see\n",
      " pay\n",
      " yo\n",
      "Cluster 16:\n",
      " ...\n",
      " /\n",
      " busi\n",
      " …\n",
      " get\n",
      " sport\n",
      " .\n",
      " ’\n",
      " health\n",
      " good\n",
      "Cluster 17:\n",
      " .\n",
      " /\n",
      " busi\n",
      " …\n",
      " sport\n",
      " peopl\n",
      " health\n",
      " good\n",
      " u\n",
      " need\n",
      "Cluster 18:\n",
      " 19\n",
      " covid\n",
      " /\n",
      " health\n",
      " …\n",
      " .\n",
      " case\n",
      " corona\n",
      " viru\n",
      " statist\n",
      "Cluster 19:\n",
      " ?\n",
      " /\n",
      " busi\n",
      " …\n",
      " .\n",
      " health\n",
      " get\n",
      " know\n",
      " want\n",
      " would\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vect.get_feature_names()\n",
    "for i in range(20):\n",
    "    print (\"Cluster %d:\" % i,)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print (' %s' % terms[ind],)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> References </B>\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python</br>\n",
    "https://datascienceplus.com/twitter-analysis-with-python/</br>\n",
    "https://berkeley-stat159-f17.github.io/stat159-f17/lectures/11-strings/11-nltk..html</br>\n",
    "https://www.xspdf.com/resolution/52940753.html</br>\n",
    "https://medium.com/analytics-vidhya/exploring-twitter-data-using-python-af1287ee65f1</br>\n",
    "https://towardsdatascience.com/kmeans-clustering-for-classification-74b992405d0a\n",
    "https://www.xspdf.com/resolution/55501459.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Annex</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watermark\n",
      "  Downloading watermark-2.1.0-py2.py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: ipython in c:\\users\\user\\anaconda3\\lib\\site-packages (from watermark) (7.16.1)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (0.4.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (3.0.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (0.17.1)\n",
      "Requirement already satisfied: pygments in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (2.6.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (4.3.3)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython->watermark) (49.2.0.post20200714)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->watermark) (0.7.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->watermark) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\user\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install watermark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %watermark [-a AUTHOR] [-d] [-n] [-t] [-i] [-z] [-u] [-c CUSTOM_TIME] [-v] [-p PACKAGES] [-h] [-m] [-g] [-r]\n",
       "                 [-b] [-w] [-iv]\n",
       "\n",
       "IPython magic function to print date/time stamps\n",
       "and various system information.\n",
       "\n",
       "optional arguments:\n",
       "  -a AUTHOR, --author AUTHOR\n",
       "                        prints author name\n",
       "  -d, --date            prints current date as YYYY-mm-dd\n",
       "  -n, --datename        prints date with abbrv. day and month names\n",
       "  -t, --time            prints current time as HH-MM-SS\n",
       "  -i, --iso8601         prints the combined date and time including the time zone in the ISO 8601 standard with UTC\n",
       "                        offset\n",
       "  -z, --timezone        appends the local time zone\n",
       "  -u, --updated         appends a string \"Last updated: \"\n",
       "  -c CUSTOM_TIME, --custom_time CUSTOM_TIME\n",
       "                        prints a valid strftime() string\n",
       "  -v, --python          prints Python and IPython version\n",
       "  -p PACKAGES, --packages PACKAGES\n",
       "                        prints versions of specified Python modules and packages\n",
       "  -h, --hostname        prints the host name\n",
       "  -m, --machine         prints system and machine info\n",
       "  -g, --githash         prints current Git commit hash\n",
       "  -r, --gitrepo         prints current Git remote address\n",
       "  -b, --gitbranch       prints current Git branch\n",
       "  -w, --watermark       prints the current version of watermark\n",
       "  -iv, --iversions      prints the name/version of all imported modules\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\anaconda3\\lib\\site-packages\\watermark\\watermark.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%watermark?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.3\n",
      "IPython version      : 7.16.1\n",
      "\n",
      "pandas : 1.0.5\n",
      "numpy  : 1.18.5\n",
      "plt    : not installed\n",
      "kmeans : not installed\n",
      "counter: not installed\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p  pandas,numpy,plt,kmeans,counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.3\n",
      "IPython version      : 7.16.1\n",
      "\n",
      "TfidfVectorizer  : not installed\n",
      "CountVectorizer  : not installed\n",
      "text             : not installed\n",
      "KMeansClusterer  : not installed\n",
      "cosine_similarity: not installed\n",
      "TruncatedSVD     : not installed\n",
      "datetime         : unknown\n",
      "nltk             : 3.5\n",
      "PCA              : not installed\n",
      "pd               : not installed\n",
      "np               : not installed\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p TfidfVectorizer,CountVectorizer,text,KMeansClusterer,cosine_similarity,TruncatedSVD,datetime,nltk,PCA,pd,np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.3\n",
      "IPython version      : 7.16.1\n",
      "\n",
      "display: not installed\n",
      "seaborn: 0.10.1\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p  display,seaborn "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
